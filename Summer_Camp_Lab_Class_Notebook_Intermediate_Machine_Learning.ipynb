{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Summer Camp Lab 2 - Intermediate Machine Learning\n",
        "\n",
        "In this lab we will practice:\n",
        "\n",
        "Recap from last lab:\n",
        "* Loading Data\n",
        "* Data Exploration\n",
        "* Selecting the Prediction Target\n",
        "* Choosing Features\n",
        "* Splitting Data into Training and Test Sets\n",
        "\n",
        "New this lab:\n",
        "* Gradient Boosted Trees (XGBoost)\n",
        "* Encoding of Categorical Variables\n",
        "* Scaling of Numerical Variables\n",
        "* Imputing Missing Values of Categorical and Numerical Variables\n",
        "* Building the Pipeline\n",
        "* Cross Validation\n",
        "* Predictions with Cross Validated Estimates"
      ],
      "metadata": {
        "id": "83e8WGupd2Lb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To work in the notebook, first copy the notebook to your own drive. File > \"Save a copy in Drive\"**"
      ],
      "metadata": {
        "id": "pmQ-JnsKlH9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up the Workspace"
      ],
      "metadata": {
        "id": "O_bQwf8uCCWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Metrics\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict"
      ],
      "metadata": {
        "id": "kOySuqgH4TgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study\n",
        "\n",
        "BigMart's team of data scientists has gathered sales data for the year 2013, encompassing 1559 products distributed across 10 stores situated in various cities. The dataset includes specific attributes for each product and store.\n",
        "\n",
        "The primary objective is to construct a predictive model capable of forecasting the sales of individual products within specific outlets.\n",
        "\n",
        "This predictive model aims to unveil the influential factors that contribute to increased sales, enabling BigMart to gain insights into product and outlet characteristics crucial for sales growth."
      ],
      "metadata": {
        "id": "lJ1zOHJh4ZIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data has the following features that could be useful in your model:\n",
        "\n",
        "* Item_Identifier: A unique identifier for each product.\n",
        "\n",
        "* Item_Weight: The weight of the product.\n",
        "\n",
        "* Item_Fat_Content: Indicates the level of fat content in the product, often categorized as 'Low Fat,' 'Regular,' etc.\n",
        "\n",
        "* Item_Visibility: The percentage of total display area of all products in a store allocated to a particular product.\n",
        "\n",
        "* Item_Type: The category or type of the product (e.g., dairy, meat, fruits, etc.).\n",
        "\n",
        "* Item_MRP (Maximum Retail Price): The maximum price at which the product can be sold.\n",
        "\n",
        "* Outlet_Identifier: A unique identifier for each store/outlet.\n",
        "\n",
        "* Outlet_Establishment_Year: The year in which the store was established.\n",
        "\n",
        "* Outlet_Size: The size of the store, often categorized as 'Small,' 'Medium,' or 'Large.'\n",
        "\n",
        "* Outlet_Location_Type: The type of location where the store is situated, such as 'Urban,' 'Suburban,' or 'Rural.'\n",
        "\n",
        "* Outlet_Type: The type of outlet, such as 'Supermarket Type1,' 'Supermarket Type2,' 'Grocery Store,' etc.\n",
        "\n",
        "* Item_Outlet_Sales: The target variable, representing the sales of the product in a particular store.\n"
      ],
      "metadata": {
        "id": "-LilGM_1o5he"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Data"
      ],
      "metadata": {
        "id": "lDKUPq_gCXfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import pandas as pd\n",
        "df_sales = pd.read_csv('https://www.dropbox.com/s/yqaymhdf7bvvair/bigmart_sales_predictions.csv?dl=1')\n",
        "\n",
        "#Fix different spelling variants of Fat Content\n",
        "df_sales['Item_Fat_Content'] = df_sales['Item_Fat_Content'].replace({'LF' : 'Low Fat', 'low fat' : 'Low Fat', 'reg' : 'Regular'})\n",
        "\n",
        "\n",
        "df_sales.head()"
      ],
      "metadata": {
        "id": "1olkYpeKnlEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration"
      ],
      "metadata": {
        "id": "A7_8InxaJ-rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales.tail()"
      ],
      "metadata": {
        "id": "ej6b-8leClxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales['Item_Fat_Content'].value_counts()"
      ],
      "metadata": {
        "id": "5YLoq3RiiJiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales.info()"
      ],
      "metadata": {
        "id": "YpcwBpXuDWcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales.describe().T"
      ],
      "metadata": {
        "id": "Ot_XYakUFVvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales.value_counts('Outlet_Location_Type')"
      ],
      "metadata": {
        "id": "7GhTOY6UHz68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting the Prediction Target"
      ],
      "metadata": {
        "id": "ZdD3SJ0KrG6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our target variable is the sales of an item at an outlet.\n",
        "y = df_sales['Item_Outlet_Sales']\n",
        "y"
      ],
      "metadata": {
        "id": "xRgznLASEg-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choosing Features"
      ],
      "metadata": {
        "id": "107Ec2NaEiGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We include a few features that we think could be useful as features in our model.\n",
        "X = df_sales[['Item_Visibility','Item_MRP','Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']]\n",
        "X"
      ],
      "metadata": {
        "id": "Ixqs6iKCrEN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.describe()"
      ],
      "metadata": {
        "id": "XoUup4qSI67T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.dtypes"
      ],
      "metadata": {
        "id": "z7JEpPwXI_Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data into Training and Test Sets\n",
        "\n",
        "Use random state if you want to generate the same split for each run of your code."
      ],
      "metadata": {
        "id": "4i5klzaorjsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "w_BPnVM2rnfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGboost"
      ],
      "metadata": {
        "id": "KV1ZcOitZQVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  from xgboost import XGBRegressor\n",
        "\n",
        "# Create a decision tree regressor\n",
        "model = XGBRegressor()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train[['Item_Visibility','Item_Weight','Item_MRP']], y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(X_test[['Item_Visibility','Item_Weight','Item_MRP']])\n",
        "\n",
        "# Evaluate the model\n",
        "print('MSE', mean_squared_error(y_test, predictions))"
      ],
      "metadata": {
        "id": "iqg0UetIZGL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the number of estimators to 10 instead of the default 100, reduces overfitting and increases the test accuracy on this dataset."
      ],
      "metadata": {
        "id": "UexxPu-bvuwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Create a decision tree regressor\n",
        "model = XGBRegressor(n_estimators=10)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train[['Item_Visibility','Item_Weight','Item_MRP']], y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(X_test[['Item_Visibility','Item_Weight','Item_MRP']])\n",
        "\n",
        "# Evaluate the model\n",
        "print('MSE', mean_squared_error(y_test, predictions))"
      ],
      "metadata": {
        "id": "jjMEgzLhvdhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Create a decision tree regressor\n",
        "model = XGBRegressor(n_estimators=100)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train[['Item_Visibility','Item_Weight','Item_MRP']], y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(X_test[['Item_Visibility','Item_Weight','Item_MRP']])\n",
        "\n",
        "# Evaluate the model\n",
        "print('MSE', mean_squared_error(y_test, predictions))"
      ],
      "metadata": {
        "id": "SRnYvuRsvmL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = X_test[['Item_Visibility','Item_Weight','Item_MRP']].copy()\n",
        "data['actual_sales'] = y_test\n",
        "data['predicted_sales'] = predictions\n",
        "data.to_csv('predictions.csv')\n",
        "data"
      ],
      "metadata": {
        "id": "OouLFX7UbfmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# @title actual_sales vs predicted_sales\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "data.plot(kind='scatter', x='actual_sales', y='predicted_sales', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "zL_uI9eBcBc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoders and Scalers"
      ],
      "metadata": {
        "id": "J5C8E88GHCCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "qmTcNL2bHBkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OrdinalEncoder"
      ],
      "metadata": {
        "id": "7tTtHnRcH201"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales[['Outlet_Size']].value_counts()"
      ],
      "metadata": {
        "id": "qnoMNaXJHksk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[['Outlet_Size']]"
      ],
      "metadata": {
        "id": "0-9zGEgWI_wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "ord_enc = OrdinalEncoder(handle_unknown='error')\n",
        "ord_enc.fit_transform(X_train[['Outlet_Size']])\n",
        "ord_enc.transform(X_test[['Outlet_Size']])"
      ],
      "metadata": {
        "id": "R-sTcMjhHHHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ordinal Encoder automatically defines an order based on the natural ordering of the categories, so for Tier 1 to 3, it encodes this as 0-2.\n",
        "However you can also specify the categories manually.\n",
        "\n",
        "Note that you define the categories as a list for each column. So `[column1, column2, ...., column3]`"
      ],
      "metadata": {
        "id": "YFS8XxqfJoJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ord_enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, categories=[ ['Small', 'Medium', 'High'] ])\n",
        "ord_enc.fit_transform(X_train[['Outlet_Size']])\n",
        "ord_enc.transform(X_test[['Outlet_Size']])"
      ],
      "metadata": {
        "id": "Rf75URFNHqjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you training data has a category that is not seen during training (`fit_transform`) then `OrdinalEncoder` will give an error with the default `handle_unknown='error'`"
      ],
      "metadata": {
        "id": "RjpHWb5fIMxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ord_enc = OrdinalEncoder(handle_unknown='error')\n",
        "ord_enc.fit_transform(X_train[['Outlet_Size']])\n",
        "ord_enc.transform(X_test[['Outlet_Size']])"
      ],
      "metadata": {
        "id": "eYRhG0H4H9T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With `handle_unkown='use_encoded_value'` the encoder will set the number or `np.nan` you set in `unknown_value`"
      ],
      "metadata": {
        "id": "hJirEtTfIedp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ord_enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "ord_enc.fit_transform(X_train[['Outlet_Size']])\n",
        "ord_enc.transform([['Unknown category'], ['Unknown']])"
      ],
      "metadata": {
        "id": "jPa5JANQIGhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The other two parameters `min_frequency` and `max_categories` can be used to purge less frequently occuring categories."
      ],
      "metadata": {
        "id": "NcSO0i-XIx3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OneHotEncoder"
      ],
      "metadata": {
        "id": "xUder33hIuKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You use `fit_transform` in the first pass of the data, the encoder then learns the categories on the training data.\n",
        "\n",
        "In subsequent uses you can use `transform` to encode each category exactly like the data you trained on."
      ],
      "metadata": {
        "id": "Gbck0UbNK4ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "oh_enc = OneHotEncoder(handle_unknown='error', sparse_output=False)\n",
        "oh_enc.fit_transform(X_train[['Outlet_Size']])\n",
        "oh_enc.transform(X_test[['Outlet_Size']])"
      ],
      "metadata": {
        "id": "E9ci0zHBJHjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can again manually specify the order with the categories column."
      ],
      "metadata": {
        "id": "2N71JsSmJnKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oh_enc = OneHotEncoder(handle_unknown='error', sparse_output=False, categories=[[np.nan, 'Small', 'Medium', 'High']])\n",
        "oh_enc.fit_transform(X_train[['Outlet_Size']])\n",
        "oh_enc.transform(X_test[['Outlet_Size']])"
      ],
      "metadata": {
        "id": "Kc739iqqJe5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As one-hot encoding really lost it's effectivenes if you have a high number of columns you can use `max_categories` and `min_frequency` to specify the minimum frequency"
      ],
      "metadata": {
        "id": "EYTUX1zxKdyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "oh_enc = OneHotEncoder(sparse_output=False, max_categories=2)\n",
        "oh_enc.fit_transform(X_train[['Outlet_Size']])\n",
        "oh_enc.transform(X_test[['Outlet_Size']])"
      ],
      "metadata": {
        "id": "mO4Ld0pWKdQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "oh_enc = OneHotEncoder(sparse_output=False, min_frequency=0.8)\n",
        "oh_enc.fit_transform(X_train[['Outlet_Size']])\n",
        "oh_enc.transform(X_test[['Outlet_Size']])"
      ],
      "metadata": {
        "id": "40FwNO0oKs3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## StandardScaler\n",
        "\n",
        "Standardize features by removing the mean and scaling to unit variance."
      ],
      "metadata": {
        "id": "3ZPZNJkdNh_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit_transform(X_train[['Item_MRP']])\n",
        "scaler.transform(X_test[['Item_MRP']])"
      ],
      "metadata": {
        "id": "7tvu-mXcNkDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MinMaxScaler\n",
        "\n",
        "Transform features by scaling each feature to a given range.\n"
      ],
      "metadata": {
        "id": "OU8Mz05fN2GR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit_transform(X_train[['Item_MRP']])\n",
        "scaler.transform(X_test[['Item_MRP']])"
      ],
      "metadata": {
        "id": "czWl1x0BNvqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imputers\n",
        "\n",
        "You can use the imputer in multiple ways, for example you can set a constant value, by using the constant strategy. `fill_value` can be a string, any number or None/np.nan."
      ],
      "metadata": {
        "id": "ZQ119eZXOTi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[['Item_Weight']]"
      ],
      "metadata": {
        "id": "zY7dHd34QbH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "imputer = SimpleImputer(strategy='constant', fill_value=np.nan)\n",
        "imputer.fit_transform(X_train[['Item_Weight']])\n",
        "imputer.transform(X_test[['Item_Weight']])"
      ],
      "metadata": {
        "id": "JBJaMih5OTVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For numerical values you can also set the missing values to the median or mean of the data given in `fit_transform`"
      ],
      "metadata": {
        "id": "zAlQi0QiP1Iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = SimpleImputer(strategy='median')\n",
        "imputer.fit_transform(X_train[['Item_MRP']])\n",
        "imputer.transform(X_test[['Item_MRP']])"
      ],
      "metadata": {
        "id": "CivlpU4_P0g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = SimpleImputer(strategy='mean')\n",
        "imputer.fit_transform(X_train[['Item_MRP']])\n",
        "imputer.transform(X_test[['Item_MRP']])"
      ],
      "metadata": {
        "id": "RwoHwRGSQZxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also set the missing values to the most frequent category seen in the data during `fit_transform`.\n",
        "\n"
      ],
      "metadata": {
        "id": "3WQ36Zn6Q7I1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "imputer.fit_transform(X_train[['Outlet_Size']])\n",
        "imputer.transform(X_test[['Outlet_Size']])"
      ],
      "metadata": {
        "id": "awZe317zQoMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[['Outlet_Size']]"
      ],
      "metadata": {
        "id": "HFF-8TcHRNBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Pipeline"
      ],
      "metadata": {
        "id": "diSJ6EynCy6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a pipeline that transforms numerical data of each column, imputes missing value with the median of the training data (`fit_transform`) and scales the data using `StandardScaler`"
      ],
      "metadata": {
        "id": "EvSl1YiMR6nK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Preprocessing for numerical data\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])"
      ],
      "metadata": {
        "id": "rw8cS0k9C5Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a pipeline that transforms categorical data of each column into a series of OneHot encoded columns."
      ],
      "metadata": {
        "id": "fK8seqYtR6MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing for categorical data\n",
        "# Raise an error if validation data contains classes that aren't represented in the training data\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "   ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "   ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "# The Pipeline() function is like a railway track with a list of different stations (steps)\n",
        "# Each step is a tuple declaring the name of the step and then the function to apply\n",
        "\n",
        "categorical_transformer"
      ],
      "metadata": {
        "id": "ea5fBss8Rnpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the column transformer so categorical columns that we specify in the list `categorical_cols` are being send to the `categorial_transformer` pipeline and `numerical_cols` are being send to the `numerical_cols`."
      ],
      "metadata": {
        "id": "YnLqeysER5zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "\n",
        "numerical_cols   = ['Item_Visibility','Item_MRP', 'Item_Weight']\n",
        "categorical_cols = ['Outlet_Size','Outlet_Location_Type', 'Outlet_Type', 'Item_Fat_Content', 'Item_Type']\n",
        "\n",
        "# Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer,   numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ], remainder='passthrough')\n",
        "    # The ColumnTransformer() function is like a railway switch: it tells what to do with the specified trainwagons (data columns).\n",
        "    # The transformers list gives the different branches where columns  can go.\n",
        "    # Each transformer is a tuple declaring the name of the transformer, the transformer to apply (eg. Pipeline defined above), and which columns need to be transformed\n",
        "    # By default the ColumnTransformer() drops every column which is not explicitly specified in the list of transformers.\n",
        "    # With the parameter remainder='passthrough', the columns that you do not mention will not be dropped (and also will not transformed).\n",
        "\n",
        "preprocessor"
      ],
      "metadata": {
        "id": "UzfFFjiHR5f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a Random Forest model to use in our experiment."
      ],
      "metadata": {
        "id": "6QmRqkRR69Bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor(random_state=0)"
      ],
      "metadata": {
        "id": "TcISvJGCSh_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a pipeline that uses our preprocessing pipeline and feeds the output into our model"
      ],
      "metadata": {
        "id": "8BzbvRoqSj3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bundle preprocessing and modeling code in a pipeline\n",
        "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                              ('model', model)\n",
        "                             ])\n",
        "                            # Here the Pipeline() function is again like a railway track, with a higer level list of different stations (steps)\n",
        "                            # Each step is a tuple declaring the name of the step and then the function to apply\n",
        "my_pipeline"
      ],
      "metadata": {
        "id": "fzlIrkVwSnGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing of training data, fit model\n",
        "my_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Preprocessing of validation data, get predictions\n",
        "preds = my_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "score = mean_absolute_error(y_test, preds)\n",
        "print('MAE:', score)\n",
        "print('MAPE:', mean_absolute_percentage_error(y_test, preds))\n",
        "print('RMSE:', mean_squared_error(y_test, preds,squared=False))"
      ],
      "metadata": {
        "id": "snFN8FlkSoHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross validation"
      ],
      "metadata": {
        "id": "Eb4zapfkUA3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing cross validation on our pipeline and data."
      ],
      "metadata": {
        "id": "BL4ClNAeVEYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiply by -1 since sklearn calculates *negative* MAE\n",
        "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
        "                              cv=5,\n",
        "                              scoring='neg_mean_absolute_error')\n",
        "print(\"MAE scores:\\n\", scores)"
      ],
      "metadata": {
        "id": "8yizlJ6PUAqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictions after cross validation\n",
        "\n",
        "You can either use `cross_val_predict`"
      ],
      "metadata": {
        "id": "L4Miw_qdcobd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cross_val_predict(my_pipeline, X, y, cv=5)"
      ],
      "metadata": {
        "id": "BB4AfuW-cBJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or retrain your pipeline with the training data."
      ],
      "metadata": {
        "id": "D5vJozeUcuD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "my_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = my_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print('MSE', mean_squared_error(y_test, predictions))"
      ],
      "metadata": {
        "id": "JLrZtSN0cl7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = X_test.copy()\n",
        "data['actual_sales'] = y_test\n",
        "data['predicted_sales'] = predictions\n",
        "data.to_csv('predictions.csv')\n",
        "data"
      ],
      "metadata": {
        "id": "4-5PD2Wpdtwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title actual_sales vs predicted_sales\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "data.plot(kind='scatter', x='actual_sales', y='predicted_sales', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "metadata": {
        "id": "PzAJm38Xdy78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "\n",
        "Write a pipeline that.\n",
        "\n",
        "- Ordinal encodes the columns `Outlet_Size`, `Outlet_Type`,\t`Outlet_Location_Type`, in the correct order of the values. (eg. Small 0 and High is 2)\n",
        "- One-Hot encodes the columns `Item_Type`. Only allow the 25 categories with the most values.\n",
        "\n",
        "- Imputes the missing values in column `Item_Weight` with the mean value, and scales them with MinMaxScaler.\n",
        "\n",
        "- Imputes the missing values in column `Item_Visibility`\tand `Item_MRP` with with the median value and scales them with StandardScaler\n",
        "\n",
        "- Use this preprocessing pipeline with a `RandomForestRegressor` mnodel and with a `XGBRegressor` model\n",
        "\n",
        "- Cross validate both model pipelines.\n",
        "- Generate cross-validated estimates with both model pipelines.\n"
      ],
      "metadata": {
        "id": "3Ccu1GVZ_lZp"
      }
    }
  ]
}